import os
import re
import json
from openai import OpenAI
from dotenv import load_dotenv
import time
from datetime import datetime

load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
files_dir = "./bigquery_docs_crawled"
jsonl_filename = "generate_bigquery_qa.jsonl"

# API í˜¸ì¶œ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
API_DELAY = 1


# ì§ˆë¬¸-ë‹µë³€ ë° ì¶œì²˜ ìƒì„± í•¨ìˆ˜
def generate_qa_and_sources(text, filename):
    """BigQuery ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ QA ìƒì„±"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ì£¼ì–´ì§„ BigQuery REST API ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ìš©í•œ ì§ˆë¬¸-ë‹µë³€ê³¼ í•´ë‹¹ ë‹µë³€ì˜ ì¶œì²˜ URLì„ ì°¾ì•„ì£¼ì„¸ìš”.

**ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:**
- ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ ì¶”ì¸¡, ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì€ ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì°¸ì¡°í•´ì•¼ í•©ë‹ˆë‹¤
- í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

**ìš°ì„ ì ìœ¼ë¡œ ë‹¤ë£° ì£¼ì œ (BigQuery íŠ¹í™”):**
1. BigQuery API ì—”ë“œí¬ì¸íŠ¸ì™€ HTTP ë©”ì„œë“œ ì‚¬ìš©ë²•
2. ìš”ì²­/ì‘ë‹µ íŒŒë¼ë¯¸í„°ì™€ ìŠ¤í‚¤ë§ˆ ì„¤ëª…
3. ì¿¼ë¦¬ ì‘ì„± ë°©ë²•ê³¼ SQL ë¬¸ë²•
4. ë°ì´í„°ì…‹, í…Œì´ë¸”, ì‘ì—…(job) ê´€ë¦¬ ë°©ë²•
5. ì¸ì¦, ê¶Œí•œ, í• ë‹¹ëŸ‰ ê´€ë ¨ ì •ë³´
6. ì½”ë“œ ì˜ˆì‹œì™€ êµ¬í˜„ íŒ¨í„´
7. ì˜¤ë¥˜ ì½”ë“œì™€ í•´ê²° ë°©ë²•
8. ì„±ëŠ¥ ìµœì í™”ì™€ ë¹„ìš© ê´€ë¦¬

**ìƒì„± ê·œì¹™:**
- ë¬¸ì„œì—ì„œ ìœ„ ì£¼ì œì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ì¶©ë¶„íˆ ìˆì„ ë•Œë§Œ ì§ˆë¬¸-ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
- API ë ˆí¼ëŸ°ìŠ¤ ë¬¸ì„œì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ê¸°ìˆ ì ì´ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”
- ë‚´ìš©ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ì¶”ìƒì ì¸ ì„¤ëª…ë§Œ ìˆë‹¤ë©´ "ìƒì„±í•  ìˆ˜ ì—†ìŒ"ì´ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”
- ìµœëŒ€ 10ê°œì˜ ì§ˆë¬¸-ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”
- ê° ì§ˆë¬¸-ë‹µë³€ì€ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤

**ì¶œì²˜ URL ì°¾ê¸° ê·œì¹™:**
1. ë¬¸ì„œ ë§¨ ìœ„ì— ìˆëŠ” Source URL (ë³´í†µ https://cloud.google.comìœ¼ë¡œ ì‹œì‘)ì„ ê¸°ë³¸ìœ¼ë¡œ í¬í•¨
2. ë‹µë³€ ë‚´ìš©ê³¼ ì§ì ‘ ê´€ë ¨ëœ íŠ¹ì • API ë©”ì„œë“œë‚˜ ë¦¬ì†ŒìŠ¤ì˜ URLì´ ë¬¸ì„œ ë‚´ì— ë³„ë„ë¡œ ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´ ì¶”ê°€ë¡œ í¬í•¨
3. ë‚´ë¶€ ë§í¬ë‚˜ ì°¸ì¡° ë§í¬ê°€ ë‹µë³€ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´ í¬í•¨
4. ë¬¸ì„œì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” URLë§Œ ì‚¬ìš©í•˜ì„¸ìš”

**ì‘ë‹µ í˜•ì‹:**
ì§ˆë¬¸1: [BigQuery API ì‚¬ìš©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸]
ë‹µë³€1: [ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œìœ¼ë¡œ ì‘ì„±í•œ ë‹µë³€]
ì¶œì²˜1: [ë¬¸ì„œ ë‚´ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” URL1, URL2, ...]

ì§ˆë¬¸2: [ë‘ ë²ˆì§¸ ì§ˆë¬¸]
ë‹µë³€2: [ë‘ ë²ˆì§¸ ë‹µë³€]
ì¶œì²˜2: [URL1, URL2, ...]

(ìµœëŒ€ 10ê°œê¹Œì§€)

ë§Œì•½ ì ì ˆí•œ API ì‚¬ìš©ë²•, íŒŒë¼ë¯¸í„° ì„¤ëª…, ì½”ë“œ ì˜ˆì‹œê°€ ë¬¸ì„œì— ì¶©ë¶„íˆ ì—†ë‹¤ë©´ "ìƒì„±í•  ìˆ˜ ì—†ìŒ"ì´ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.""",
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ BigQuery REST API ë¬¸ì„œë¥¼ ì •í™•íˆ ë¶„ì„í•´ì„œ, API ì‚¬ìš©ë²•, íŒŒë¼ë¯¸í„°, ì½”ë“œ ì˜ˆì‹œ ë“± ì‹¤ìš©ì ì¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœëŒ€ 10ê°œì˜ ì§ˆë¬¸-ë‹µë³€ê³¼ í•´ë‹¹ ì¶œì²˜ URLë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”. ì ì ˆí•œ ë‚´ìš©ì´ ì—†ë‹¤ë©´ 'ìƒì„±í•  ìˆ˜ ì—†ìŒ'ì´ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”:\n\níŒŒì¼ëª…: {filename}\n\n{text}",
                },
            ],
            max_tokens=2000,
            temperature=0,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"  âŒ ì§ˆë¬¸-ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def extract_urls_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ URL ì¶”ì¶œ"""
    # HTTP/HTTPS URL íŒ¨í„´
    url_pattern = r'https?://[^\s\],;)\'"]+[^\s\],;)\'".]'
    urls = re.findall(url_pattern, text)

    # URL ì •ë¦¬ (ëì— ë¶™ì€ íŠ¹ìˆ˜ë¬¸ì ì œê±°)
    cleaned_urls = []
    for url in urls:
        # ëì— ìˆëŠ” êµ¬ë‘ì  ì œê±°
        url = re.sub(r"[.,;:)]+$", "", url)
        # ì¤‘ë³µ ì œê±°
        if url not in cleaned_urls:
            cleaned_urls.append(url)

    return cleaned_urls


def parse_single_qa_block(block):
    """ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€-ì¶œì²˜ ë¸”ë¡ íŒŒì‹±"""
    try:
        qa_dict = {}
        lines = block.strip().split("\n")

        current_section = None
        question_lines = []
        answer_lines = []
        source_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì§ˆë¬¸ ì‹œì‘ íŒ¨í„´
            if re.match(r"^ì§ˆë¬¸\d*:", line):
                current_section = "question"
                question_content = re.sub(r"^ì§ˆë¬¸\d*:", "", line).strip()
                if question_content:
                    question_lines.append(question_content)

            # ë‹µë³€ ì‹œì‘ íŒ¨í„´
            elif re.match(r"^ë‹µë³€\d*:", line):
                current_section = "answer"
                answer_content = re.sub(r"^ë‹µë³€\d*:", "", line).strip()
                if answer_content:
                    answer_lines.append(answer_content)

            # ì¶œì²˜ ì‹œì‘ íŒ¨í„´
            elif re.match(r"^ì¶œì²˜\d*:", line):
                current_section = "sources"
                source_content = re.sub(r"^ì¶œì²˜\d*:", "", line).strip()
                if source_content:
                    source_lines.append(source_content)

            # ì—°ì†ë˜ëŠ” ë‚´ìš© ë¼ì¸
            else:
                if current_section == "question":
                    question_lines.append(line)
                elif current_section == "answer":
                    answer_lines.append(line)
                elif current_section == "sources":
                    source_lines.append(line)

        # ê° ì„¹ì…˜ ì¡°í•©
        if question_lines:
            qa_dict["question"] = " ".join(question_lines).strip()

        if answer_lines:
            qa_dict["answer"] = " ".join(answer_lines).strip()

        # URL ì¶”ì¶œ
        if source_lines:
            all_source_text = " ".join(source_lines)
            urls = extract_urls_from_text(all_source_text)
            # BigQuery ê´€ë ¨ URLë§Œ í•„í„°ë§
            bigquery_urls = [url for url in urls if 'bigquery' in url.lower() or 'cloud.google.com' in url]
            qa_dict["sources"] = bigquery_urls if bigquery_urls else urls if urls else ["ì¶œì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"]
        else:
            qa_dict["sources"] = ["ì¶œì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"]

        return qa_dict

    except Exception as e:
        print(f"  âš ï¸ ë‹¨ì¼ QA ë¸”ë¡ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return {}


def parse_qa_and_sources(ai_response):
    """ê°œì„ ëœ ì§ˆë¬¸-ë‹µë³€-ì¶œì²˜ íŒŒì‹± í•¨ìˆ˜"""
    try:
        # "ìƒì„±í•  ìˆ˜ ì—†ìŒ" ì‘ë‹µ ì²´í¬
        if "ìƒì„±í•  ìˆ˜ ì—†ìŒ" in ai_response:
            print("  â†’ ì ì ˆí•œ ë‚´ìš©ì´ ì—†ì–´ì„œ ì§ˆë¬¸-ë‹µë³€ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ")
            return []

        qa_pairs = []

        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì§ˆë¬¸ ë‹¨ìœ„ë¡œ ë¶„í• 
        # ì§ˆë¬¸1:, ì§ˆë¬¸2: ë“±ì˜ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        question_blocks = re.split(r"\n(?=ì§ˆë¬¸\d*:)", ai_response.strip())

        for block in question_blocks:
            if not block.strip():
                continue

            # ê° ë¸”ë¡ì—ì„œ ì§ˆë¬¸, ë‹µë³€, ì¶œì²˜ ì¶”ì¶œ
            qa_dict = parse_single_qa_block(block)
            if qa_dict and qa_dict.get("question") and qa_dict.get("answer"):
                # ë°ì´í„° ì •ë¦¬
                cleaned_qa = {
                    "question": qa_dict["question"].strip(),
                    "answer": qa_dict["answer"].strip(),
                    "sources": qa_dict.get("sources", ["ì¶œì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"]),
                }
                qa_pairs.append(cleaned_qa)

        return qa_pairs

    except Exception as e:
        print(f"  âš ï¸ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def categorize_content(filename, question):
    """íŒŒì¼ëª…ê³¼ ì§ˆë¬¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    categories = []

    filename_lower = filename.lower()
    question_lower = question.lower()

    # íŒŒì¼ëª… ê¸°ë°˜ ì¹´í…Œê³ ë¦¬
    if 'datasets' in filename_lower:
        categories.append('datasets')
    if 'tables' in filename_lower:
        categories.append('tables')
    if 'jobs' in filename_lower:
        categories.append('jobs')
    if 'queries' in filename_lower:
        categories.append('queries')
    if 'models' in filename_lower:
        categories.append('models')
    if 'routines' in filename_lower:
        categories.append('routines')
    if 'projects' in filename_lower:
        categories.append('projects')

    # ì§ˆë¬¸ ë‚´ìš© ê¸°ë°˜ ì¹´í…Œê³ ë¦¬
    if 'dataset' in question_lower or 'ë°ì´í„°ì…‹' in question_lower:
        categories.append('datasets')
    if 'table' in question_lower or 'í…Œì´ë¸”' in question_lower:
        categories.append('tables')
    if 'query' in question_lower or 'ì¿¼ë¦¬' in question_lower or 'sql' in question_lower:
        categories.append('queries')
    if 'job' in question_lower or 'ì‘ì—…' in question_lower:
        categories.append('jobs')
    if 'permission' in question_lower or 'iam' in question_lower or 'ê¶Œí•œ' in question_lower:
        categories.append('permissions')
    if 'error' in question_lower or 'exception' in question_lower or 'ì˜¤ë¥˜' in question_lower:
        categories.append('troubleshooting')

    # ì¤‘ë³µ ì œê±°
    categories = list(set(categories))

    # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ general ì¶”ê°€
    if not categories:
        categories = ['general']

    return categories


def validate_qa_pair(qa):
    """QA ìŒì˜ ìœ íš¨ì„± ê²€ì¦"""
    # ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìµœì†Œ ê¸¸ì´ í™•ì¸
    if len(qa['question']) < 10 or len(qa['answer']) < 20:
        return False

    # ë‹µë³€ì´ ë‹¨ìˆœíˆ "ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”" ê°™ì€ ë‚´ìš©ì¸ì§€ í™•ì¸
    unhelpful_patterns = [
        "ë¬¸ì„œë¥¼ ì°¸ì¡°",
        "ìì„¸í•œ ë‚´ìš©ì€",
        "ë” ì•Œì•„ë³´ë ¤ë©´",
        "ì°¸ê³ í•˜ì„¸ìš”"
    ]

    answer_lower = qa['answer'].lower()
    if any(pattern in answer_lower for pattern in unhelpful_patterns) and len(qa['answer']) < 50:
        return False

    return True


# ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
def main():
    jsonl_data = []
    total_files = 0
    processed_files = 0
    skipped_files = 0

    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    current_date = datetime.now().strftime("%Y-%m-%d")

    print("=" * 60)
    print("ğŸš€ BigQuery API ë¬¸ì„œ QA ìƒì„± ì‹œì‘")
    print("=" * 60)

    # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    txt_files = [f for f in os.listdir(files_dir) if f.endswith(".txt")]
    total_files = len(txt_files)

    print(f"ğŸ“ ì´ {total_files}ê°œì˜ ë¬¸ì„œ íŒŒì¼ ë°œê²¬\n")

    for idx, filename in enumerate(txt_files, 1):
        file_path = os.path.join(files_dir, filename)
        print(f"[{idx}/{total_files}] ì²˜ë¦¬ ì¤‘: {filename}")

        try:
            with open(file_path, "r", encoding="utf-8") as file:
                text = file.read()

            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = len(text)
            if file_size < 500:
                print(f"  âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ ({file_size} ë¬¸ì) - ê±´ë„ˆë›°ê¸°")
                skipped_files += 1
                continue

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸° (í† í° ì œí•œ ê³ ë ¤)
            if file_size > 15000:
                text = text[:15000]
                print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •: {file_size} â†’ 15000 ë¬¸ì")

            # AIê°€ ì§ˆë¬¸-ë‹µë³€ê³¼ ì¶œì²˜ë¥¼ í•œ ë²ˆì— ìƒì„±
            ai_response = generate_qa_and_sources(text, filename)

            if ai_response:
                # AI ì‘ë‹µ íŒŒì‹± (ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒ)
                qa_pairs = parse_qa_and_sources(ai_response)

                if qa_pairs:
                    valid_qa_count = 0

                    for i, qa in enumerate(qa_pairs, 1):
                        # QA ìœ íš¨ì„± ê²€ì¦
                        if not validate_qa_pair(qa):
                            print(f"    âš ï¸ QA{i} ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨ - ê±´ë„ˆë›°ê¸°")
                            continue

                        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                        categories = categorize_content(filename, qa["question"])

                        # ë©”íƒ€ë°ì´í„° ì„¤ì •
                        metadata = {
                            "question": qa["question"],
                            "answer": qa["answer"],
                            "sources": qa["sources"],
                            "tags": "bigquery,rest-api",
                            "categories": categories,
                            "api_version": "v2",
                            "last_verified": current_date,
                            "source_file": filename,
                            "doc_type": "api_reference",
                            "language": "ko"
                        }

                        jsonl_data.append(metadata)
                        valid_qa_count += 1

                        # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì²« 50ìë§Œ)
                        print(f"    âœ… QA{i}: {qa['question'][:50]}...")
                        if len(qa['sources']) > 0 and qa['sources'][0] != "ì¶œì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                            print(f"       ì¶œì²˜: {qa['sources'][0][:60]}...")

                    if valid_qa_count > 0:
                        print(f"  â†’ {valid_qa_count}ê°œì˜ ìœ íš¨í•œ ì§ˆë¬¸-ë‹µë³€ ìŒ ìƒì„±ë¨")
                        processed_files += 1
                    else:
                        print(f"  â†’ ìœ íš¨í•œ QAê°€ ì—†ì–´ì„œ ê±´ë„ˆë›°ê¸°")
                        skipped_files += 1
                else:
                    print(f"  â†’ ì ì ˆí•œ ë‚´ìš©ì´ ì—†ì–´ì„œ ê±´ë„ˆë›°ê¸°")
                    skipped_files += 1
            else:
                print(f"  âŒ AI ì‘ë‹µ ì‹¤íŒ¨")
                skipped_files += 1

        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            skipped_files += 1

        # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§€ì—°
        if idx < total_files:
            time.sleep(API_DELAY)

    # JSONL íŒŒì¼ë¡œ ì €ì¥
    if jsonl_data:
        with open(jsonl_filename, "w", encoding="utf-8") as jsonl_file:
            for item in jsonl_data:
                jsonl_file.write(json.dumps(item, ensure_ascii=False) + "\n")

        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        elapsed_time = time.time() - start_time
        minutes = int(elapsed_time // 60)
        seconds = int(elapsed_time % 60)

        # ìµœì¢… í†µê³„ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š QA ìƒì„± ì™„ë£Œ í†µê³„")
        print("=" * 60)
        print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}ê°œ")
        print(f"âš ï¸ ê±´ë„ˆë›´ íŒŒì¼: {skipped_files}ê°œ")
        print(f"ğŸ“ ìƒì„±ëœ QA ìŒ: {len(jsonl_data)}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {jsonl_filename}")
        print(f"â±ï¸ ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")
        print("=" * 60)

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for item in jsonl_data:
            for cat in item.get('categories', ['general']):
                category_stats[cat] = category_stats.get(cat, 0) + 1

        print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ QA ë¶„í¬:")
        for cat, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {cat}: {count}ê°œ")

    else:
        print("\nâš ï¸ ìƒì„±ëœ QAê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    print("\nâœ¨ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ‘‰ .env íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒê³¼ ê°™ì´ API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:")
        print("   OPENAI_API_KEY=your-api-key-here")
    elif not os.path.exists(files_dir):
        print(f"âŒ ì˜¤ë¥˜: '{files_dir}' ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ğŸ‘‰ ë¨¼ì € BigQuery ë¬¸ì„œ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        main()