import os
import json
import torch
from pathlib import Path
from typing import List, Optional
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm


class GoogleAPIDocumentProcessor:
    """êµ¬ê¸€ API ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ì‹œìŠ¤í…œ"""

    def __init__(self,
                 api_data_dir: str = "./GOOGLE_API_DATA",
                 db_dir: str = "./chroma_google_api_db_gpt"):
        """
        Args:
            api_data_dir: êµ¬ê¸€ API ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬. í•˜ìœ„ í´ë”ëª…ì„ 'tags' ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            db_dir: Chroma DB ì €ì¥ ê²½ë¡œ
        """
        self.api_data_dir = Path(api_data_dir)
        self.db_dir = db_dir

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.documents: List[Document] = []
        self.vectorstore: Optional[Chroma] = None
        self.embedding_model: Optional[HuggingFaceEmbeddings] = None

    def _get_tag_from_path(self, file_path: Path) -> str:
        """
        íŒŒì¼ ê²½ë¡œì˜ ìƒìœ„ í´ë”ëª…ì„ íƒœê·¸(ëŒ€ë¶„ë¥˜)ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ì˜ˆì‹œ:
        - ./GOOGLE_API_DATA/gmail/send_email.txt -> 'gmail'
        - ./GOOGLE_API_DATA/drive/list_files.txt -> 'drive'
        - ./GOOGLE_API_DATA/some_other_doc.txt -> 'general'
        """
        try:
            relative_path = file_path.relative_to(self.api_data_dir)
            if len(relative_path.parts) > 1:
                return relative_path.parts[0]
        except ValueError:
            pass
        return 'general'

    def load_api_documents(self) -> List[Document]:
        """
        êµ¬ê¸€ API ì›ë¬¸(.txt) ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        documents = []

        if not self.api_data_dir.exists():
            print(f"âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.api_data_dir}")
            return documents

        print(f"ğŸ“‚ API ë°ì´í„° ë¡œë“œ ì¤‘ (.txt íŒŒì¼ë§Œ íƒìƒ‰): {self.api_data_dir}")
        file_paths = list(self.api_data_dir.rglob("*.txt"))

        for file_path in file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì²­í‚¹ ë°©ì‹ ìˆ˜ì •
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1200,
                    chunk_overlap=150,
                    separators=["\n\n", "\n", ". ", " ", ""]  # ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´ ì‹œë„
                )
                chunks = text_splitter.split_text(content)

                for i, chunk in enumerate(chunks):
                    # ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ìµœì¢… ë©”íƒ€ë°ì´í„° êµ¬ì¡° ìˆ˜ì •
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            'chunk_id': i,
                            'source': str(file_path.relative_to(self.api_data_dir)),
                            'tags': self._get_tag_from_path(file_path),
                            'source_file': file_path.name,
                            'last_verified': datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d')
                        }
                    )
                    documents.append(doc)

            except Exception as e:
                print(f"âš ï¸ {file_path} íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        self.documents = documents[:1000]
        print(f"âœ… [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ì´ {len(documents)}ê°œì˜ ì²­í¬ ì¤‘ 1000ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        print(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents

    def initialize_vectorstore(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì„ë² ë”©"""
        if not self.documents:
            print("âš ï¸ ë²¡í„° DBë¥¼ ìƒì„±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `load_api_documents`ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return

        print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... (BAAI/bge-m3)")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        if os.path.exists(self.db_dir) and any(Path(self.db_dir).iterdir()):
            print(f"ğŸ’¾ ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œê°€ '{self.db_dir}'ì— ì¡´ì¬í•©ë‹ˆë‹¤.")
            user_input = input("ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            if user_input != 'y':
                print("ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
            else:
                # ê¸°ì¡´ í´ë” ì‚­ì œ
                import shutil
                shutil.rmtree(self.db_dir)
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ '{self.db_dir}' í´ë”ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

        print("ğŸ’¾ ìƒˆ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘... (ì§„í–‰ë¥  í‘œì‹œ)")

        # ì²« ë²ˆì§¸ ì²­í¬ë¡œ DB ì´ˆê¸°í™”
        self.vectorstore = Chroma.from_documents(
            documents=[self.documents[0]],  # ì²« ë¬¸ì„œ í•˜ë‚˜ë¡œë§Œ ì´ˆê¸°í™”
            embedding=self.embedding_model,
            persist_directory=self.db_dir,
        )

        # ë‚˜ë¨¸ì§€ ë¬¸ì„œë¥¼ tqdmìœ¼ë¡œ ì§„í–‰ë¥ ì„ ë³´ë©° ì¶”ê°€
        batch_size = 100  # í•œ ë²ˆì— 100ê°œì”© ì¶”ê°€
        for i in tqdm(range(1, len(self.documents), batch_size), desc="ì„ë² ë”© ë° DB ì €ì¥ ì¤‘"):
            batch = self.documents[i:i + batch_size]
            self.vectorstore.add_documents(batch)

        print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ ({self.db_dir})")

    def build_database(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° DB êµ¬ì¶•"""
        print("=" * 60)
        print("ğŸš€ API ë¬¸ì„œ ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
        print("=" * 60)

        self.load_api_documents()
        self.initialize_vectorstore()

        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 60 + "\n")


# --- ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    # ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì´ì œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ë²¡í„° DBë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ë§Œ í•©ë‹ˆë‹¤.
    try:
        # ë°ì´í„°ê°€ ì €ì¥ëœ ìƒìœ„ í´ë” ë° DBë¥¼ ì €ì¥í•  ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
        processor = GoogleAPIDocumentProcessor(
            api_data_dir='../GOOGLE_API_DATA',
            db_dir='../chroma_google_api_db_gpt'
        )
        # DB êµ¬ì¶• ì‹¤í–‰
        processor.build_database()

    except Exception as e:
        print(f"ğŸ’¥ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")