import os
import json
import torch
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Literal
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import openai


class GoogleAPIRAGSystem:
    """êµ¬ê¸€ API ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ RAG ì‹œìŠ¤í…œ (GPT-4o & Qwen3:8B)"""

    def __init__(self,
                 api_data_dir: str = "./GOOGLE_API_DATA",
                 api_qa_dir: str = "./GOOGLE_API_DATA/GOOGLE_API_DATA_QA",
                 db_dir: str = "./chroma_google_api_db",
                 openai_api_key: Optional[str] = None):
        """
        Args:
            api_data_dir: êµ¬ê¸€ API ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬
            api_qa_dir: êµ¬ê¸€ API QA ë°ì´í„° ë””ë ‰í† ë¦¬
            db_dir: Chroma DB ì €ì¥ ê²½ë¡œ
            openai_api_key: OpenAI API í‚¤ (GPT-4o ì‚¬ìš©ì‹œ)
        """
        self.api_data_dir = Path(api_data_dir)
        self.api_qa_dir = Path(api_qa_dir)
        self.db_dir = db_dir

        # OpenAI ì„¤ì • (GPT-4oìš©)
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
            openai.api_key = openai_api_key

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.documents = []
        self.vectorstore = None
        self.retriever = None

        # ëª¨ë¸ ê´€ë ¨
        self.qwen_model = None
        self.qwen_tokenizer = None
        self.gpt4o_model = None
        self.embedding_model = None

        # í˜„ì¬ ì‚¬ìš© ëª¨ë¸
        self.current_model: Literal["gpt4o", "qwen"] = "qwen"

    def load_api_documents(self) -> List[Document]:
        """êµ¬ê¸€ API ì›ë¬¸ ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  Document ê°ì²´ë¡œ ë³€í™˜"""
        documents = []

        # GOOGLE_API_DATA í´ë”ì—ì„œ ì›ë³¸ API ë¬¸ì„œë§Œ ë¡œë“œ
        if self.api_data_dir.exists():
            print(f"ğŸ“‚ ì›ë³¸ API ë°ì´í„° ë¡œë“œ ì¤‘: {self.api_data_dir}")

            # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì²­í‚¹í•˜ì—¬ ë¡œë“œ
            for file_path in self.api_data_dir.glob("*.txt"):
                if file_path.parent == self.api_data_dir:  # í•˜ìœ„ í´ë” ì œì™¸
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # í…ìŠ¤íŠ¸ ì²­í‚¹
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=1500,
                            chunk_overlap=300,
                            separators=["\n\n\n", "\n\n", "\n", ".", " "]
                        )

                        chunks = text_splitter.split_text(content)

                        for i, chunk in enumerate(chunks):
                            doc = Document(
                                page_content=chunk,
                                metadata={
                                    'type': 'api_doc',
                                    'source_file': file_path.name,
                                    'chunk_id': i,
                                    'api_category': self._extract_api_category(file_path.name)
                                }
                            )
                            documents.append(doc)

                    except Exception as e:
                        print(f"âš ï¸ Error loading {file_path}: {e}")

            # JSON íŒŒì¼ë“¤ë„ ë¡œë“œ (ì›ë³¸ API ë°ì´í„°ì¸ ê²½ìš°)
            for file_path in self.api_data_dir.glob("*.json"):
                if file_path.parent == self.api_data_dir:  # í•˜ìœ„ í´ë” ì œì™¸
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            json_data = json.load(f)

                        # JSON ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        content = json.dumps(json_data, ensure_ascii=False, indent=2)

                        # í…ìŠ¤íŠ¸ ì²­í‚¹
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=1500,
                            chunk_overlap=300,
                            separators=["\n\n", "\n", "}", ","]
                        )

                        chunks = text_splitter.split_text(content)

                        for i, chunk in enumerate(chunks):
                            doc = Document(
                                page_content=chunk,
                                metadata={
                                    'type': 'api_doc',
                                    'source_file': file_path.name,
                                    'chunk_id': i,
                                    'api_category': self._extract_api_category(file_path.name)
                                }
                            )
                            documents.append(doc)

                    except Exception as e:
                        print(f"âš ï¸ Error loading {file_path}: {e}")

        # ìƒ˜í”Œ ì›ë¬¸ ë°ì´í„° ì¶”ê°€ (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°)
        if not documents:
            documents = self._create_sample_api_documents()

        self.documents = documents
        print(f"âœ… ì´ {len(documents)}ê°œì˜ ì›ë¬¸ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents

    def _extract_api_category(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ API ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        filename_lower = filename.lower()

        # êµ¬ê¸€ API ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        api_categories = {
            'gmail': 'gmail',
            'drive': 'drive',
            'calendar': 'calendar',
            'sheets': 'sheets',
            'docs': 'docs',
            'slides': 'slides',
            'meet': 'meet',
            'maps': 'maps',
            'youtube': 'youtube',
            'analytics': 'analytics'
        }

        for key in api_categories:
            if key in filename_lower:
                return api_categories[key]

        return 'general'

    def _extract_api_category_from_content(self, content: str) -> str:
        """ë‚´ìš©ì—ì„œ API ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        content_lower = content.lower()

        api_keywords = {
            'gmail': ['gmail', 'ì´ë©”ì¼', 'email', 'messages.send'],
            'drive': ['drive', 'ë“œë¼ì´ë¸Œ', 'files.list', 'files.create'],
            'calendar': ['calendar', 'ìº˜ë¦°ë”', 'events.insert', 'events.list'],
            'sheets': ['sheets', 'ìŠ¤í”„ë ˆë“œì‹œíŠ¸', 'spreadsheet', 'values.update'],
            'docs': ['docs', 'ë¬¸ì„œ', 'documents.create'],
            'youtube': ['youtube', 'ìœ íŠœë¸Œ', 'videos.list'],
            'maps': ['maps', 'ì§€ë„', 'geocoding', 'directions']
        }

        for category, keywords in api_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                return category

        return 'general'

    def _create_sample_api_documents(self) -> List[Document]:
        """ìƒ˜í”Œ êµ¬ê¸€ API ì›ë¬¸ ë°ì´í„° ìƒì„±"""
        sample_api_data = [
            """Gmail API Reference

messages.send
Sends the specified message to the recipients in the To, Cc, and Bcc headers.

HTTP request:
POST https://gmail.googleapis.com/gmail/v1/users/{userId}/messages/send

Parameters:
- userId: The user's email address. The special value 'me' can be used.

Request body:
The request body contains an instance of Message.

Required OAuth scope:
https://www.googleapis.com/auth/gmail.send

Example:
service.users().messages().send(userId='me', body=message).execute()""",

            """Google Drive API Reference

files.list
Lists or searches files.

HTTP request:
GET https://www.googleapis.com/drive/v3/files

Query parameters:
- q: A query for filtering the file results
- pageSize: The maximum number of files to return
- fields: The paths of the fields you want included in the response

Common search queries:
- mimeType='application/pdf' : PDF files only
- 'folder_id' in parents : Files in specific folder
- name contains 'report' : Files with 'report' in name

Example:
service.files().list(q="mimeType='application/pdf'", pageSize=10).execute()""",

            """Google Calendar API Reference

events.insert
Creates an event.

HTTP request:
POST https://www.googleapis.com/calendar/v3/calendars/{calendarId}/events

Parameters:
- calendarId: Calendar identifier. To retrieve calendar IDs use calendarList.list()

Request body:
{
  "summary": "Event title",
  "start": {"dateTime": "2024-01-15T10:00:00", "timeZone": "Asia/Seoul"},
  "end": {"dateTime": "2024-01-15T11:00:00", "timeZone": "Asia/Seoul"},
  "recurrence": ["RRULE:FREQ=WEEKLY;COUNT=10"]
}

Example:
service.events().insert(calendarId='primary', body=event).execute()"""
        ]

        documents = []
        for i, content in enumerate(sample_api_data):
            doc = Document(
                page_content=content,
                metadata={
                    'type': 'api_doc',
                    'source_file': f'sample_api_{i}.txt',
                    'chunk_id': 0,
                    'api_category': self._extract_api_category_from_content(content)
                }
            )
            documents.append(doc)

        return documents

    def initialize_vectorstore(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì„ë² ë”©"""
        print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        if os.path.exists(self.db_dir) and any(Path(self.db_dir).iterdir()):
            print(f"ğŸ’¾ ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘: {self.db_dir}")
            self.vectorstore = Chroma(
                persist_directory=self.db_dir,
                embedding_function=self.embedding_model
            )
        else:
            print("ğŸ’¾ ìƒˆ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
            self.vectorstore = Chroma.from_documents(
                documents=self.documents,
                embedding=self.embedding_model,
                persist_directory=self.db_dir,
                collection_metadata={"hnsw:space": "cosine"}
            )

        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={"k": 5, "score_threshold": 0.3}
        )
        print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ ({self.db_dir})")

    def initialize_models(self, use_gpt4o: bool = False, openai_api_key: Optional[str] = None):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""

        if use_gpt4o:
            # GPT-4o ì´ˆê¸°í™”
            if openai_api_key:
                os.environ["OPENAI_API_KEY"] = openai_api_key

            print("ğŸ¤– GPT-4o ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self.gpt4o_model = ChatOpenAI(
                model="gpt-4o",
                temperature=0.7,
                max_tokens=1024
            )
            self.current_model = "gpt4o"
            print("âœ… GPT-4o ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

        else:
            # Qwen3:8B ì´ˆê¸°í™” (ë¡œì»¬ ëª¨ë¸)
            print("ğŸ¤– Qwen3:8B ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")

            model_name = "Qwen/Qwen2.5-7B-Instruct"  # ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ì—…ê·¸ë ˆì´ë“œ
            local_model_path = "./Qwen2.5-7B-Instruct"

            if os.path.exists(local_model_path):
                print(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {local_model_path}")
                self.qwen_tokenizer = AutoTokenizer.from_pretrained(
                    local_model_path,
                    local_files_only=True
                )
                self.qwen_model = AutoModelForCausalLM.from_pretrained(
                    local_model_path,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    local_files_only=True
                )
            else:
                print(f"â¬‡ï¸ Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}")
                self.qwen_tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.qwen_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                # ë¡œì»¬ ì €ì¥
                self.qwen_tokenizer.save_pretrained(local_model_path)
                self.qwen_model.save_pretrained(local_model_path)
                print(f"ğŸ’¾ ëª¨ë¸ì´ {local_model_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            self.current_model = "qwen"
            print("âœ… Qwen3:8B ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

    def format_docs_for_context(self, docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        formatted = []

        for i, doc in enumerate(docs, 1):
            content = f"[ì°¸ê³  {i}]\n"
            content += f"ë‚´ìš©: {doc.page_content}"
            if 'api_category' in doc.metadata:
                content += f"\nì¹´í…Œê³ ë¦¬: {doc.metadata['api_category']}"
            formatted.append(content)

        return "\n\n---\n\n".join(formatted)

    def get_prompt_template(self, use_haeyoche: bool = False) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""

        if use_haeyoche:
            # Qwen ëª¨ë¸ìš© (í•´ìš”ì²´)
            template = """ë‹¹ì‹ ì€ êµ¬ê¸€ API ì „ë¬¸ê°€ì˜ˆìš”. ë‹¤ìŒì˜ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë°œìì—ê²Œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€í•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì •í™•í•œ API ë©”ì„œë“œëª…ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”
2. ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
3. í•„ìš”í•œ ê¶Œí•œì´ë‚˜ ì£¼ì˜ì‚¬í•­ì´ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
4. ì¹œê·¼í•œ í•´ìš”ì²´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”

ë‹µë³€:"""
        else:
            # GPT-4oìš© (ë©€í‹°í„´ ê³ ë ¤)
            template = """You are a Google API expert. Based on the following reference materials, provide accurate and practical answers to help developers.

Reference Materials:
{context}

Question: {question}

Please ensure your answer includes:
1. Exact API method names and parameters
2. Practical code examples
3. Required permissions or important notes
4. Clear and structured explanation

Answer in Korean:"""

        return template

    def retrieve_with_scores(self, query: str, k: int = 5) -> Tuple[List[Document], List[float]]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)"""
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        docs = [doc for doc, _ in results]
        scores = [score for _, score in results]
        return docs, scores

    def generate_response_with_gpt4o(self, query: str, context: str) -> str:
        """GPT-4oë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (ë©€í‹°í„´ ì§€ì›)"""
        messages = [
            SystemMessage(content="""ë‹¹ì‹ ì€ êµ¬ê¸€ API ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ê°œë°œìë“¤ì—ê²Œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            ì½”ë“œ ì˜ˆì‹œì™€ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""),
            HumanMessage(content=f"""
ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€í•´ì£¼ì„¸ìš”:""")
        ]

        response = self.gpt4o_model.invoke(messages)
        return response.content

    def generate_response_with_qwen(self, query: str, context: str) -> str:
        """Qwen ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (í•´ìš”ì²´)"""
        prompt_template = self.get_prompt_template(use_haeyoche=True)
        prompt_text = prompt_template.format(context=context, question=query)

        # í† í°í™”
        inputs = self.qwen_tokenizer(
            prompt_text,
            return_tensors="pt",
            max_length=2048,
            truncation=True
        ).to(self.qwen_model.device)

        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = self.qwen_model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.qwen_tokenizer.eos_token_id,
                eos_token_id=self.qwen_tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        full_response = self.qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer_only = full_response.replace(prompt_text, "").strip()

        return answer_only

    def generate_response(self, query: str, use_gpt4o: Optional[bool] = None) -> Tuple[
        List[Document], List[float], str]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""

        # ëª¨ë¸ ì„ íƒ
        if use_gpt4o is None:
            use_gpt4o = (self.current_model == "gpt4o")

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs, scores = self.retrieve_with_scores(query)

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = self.format_docs_for_context(docs)

        # ì‘ë‹µ ìƒì„±
        if use_gpt4o and self.gpt4o_model:
            response = self.generate_response_with_gpt4o(query, context)
        elif self.qwen_model:
            response = self.generate_response_with_qwen(query, context)
        else:
            response = "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize_models()ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."

        return docs, scores, response

    def initialize_all(self, use_gpt4o: bool = False, openai_api_key: Optional[str] = None):
        """ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("=" * 60)
        print("ğŸš€ Google API RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        print("=" * 60)

        # 1. ë¬¸ì„œ ë¡œë“œ
        print("\nğŸ“š [1/3] API ì›ë¬¸ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        self.load_api_documents()

        # 2. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        print("\nğŸ” [2/3] ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        self.initialize_vectorstore()

        # 3. LLM ì´ˆê¸°í™”
        print("\nğŸ¤– [3/3] LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.initialize_models(use_gpt4o=use_gpt4o, openai_api_key=openai_api_key)

        print("\n" + "=" * 60)
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“Š í˜„ì¬ ëª¨ë¸: {'GPT-4o' if self.current_model == 'gpt4o' else 'Qwen3:8B'}")
        print("=" * 60 + "\n")

    def search_api(self, query: str, use_gpt4o: Optional[bool] = None, verbose: bool = True):
        """API ê²€ìƒ‰ ë° ì‘ë‹µ ì œê³µ"""
        docs, scores, response = self.generate_response(query, use_gpt4o)

        if verbose:
            print("\n" + "=" * 60)
            print(f"ğŸ” ì§ˆë¬¸: {query}")
            print(f"ğŸ¤– ëª¨ë¸: {'GPT-4o' if (use_gpt4o or self.current_model == 'gpt4o') else 'Qwen3:8B (í•´ìš”ì²´)'}")
            print("=" * 60)

            print("\nğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:")
            for i, (doc, score) in enumerate(zip(docs[:3], scores[:3]), 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                category = doc.metadata.get('api_category', 'general')
                print(f"\n  [{i}] API ë¬¸ì„œ - {category} (ìœ ì‚¬ë„: {score:.4f})")
                print(f"      ë‚´ìš©: {doc.page_content[:80]}...")

            print("\n" + "-" * 60)
            print("ğŸ’¡ ë‹µë³€:")
            print("-" * 60)
            print(response)
            print("=" * 60 + "\n")

        return response

    def add_conversation_history(self, query: str, response: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ë©€í‹°í„´ ì§€ì›)"""
        # í–¥í›„ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬ ì €ì¥
        if not hasattr(self, 'conversation_history'):
            self.conversation_history = []

        self.conversation_history.append({
            'query': query,
            'response': response,
            'model': self.current_model
        })


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    import argparse

    # ëª…ë ¹ì¤„ ì¸ì íŒŒì„œ
    parser = argparse.ArgumentParser(description='Google API RAG System')
    parser.add_argument('--use-gpt4o', action='store_true', help='Use GPT-4o instead of Qwen')
    parser.add_argument('--api-key', type=str, help='OpenAI API key for GPT-4o')
    args = parser.parse_args()

    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = GoogleAPIRAGSystem(
        api_data_dir="../GOOGLE_API_DATA",
        api_qa_dir="../GOOGLE_API_DATA/GOOGLE_API_DATA_QA",
        db_dir="../chroma_google_api_db",
        openai_api_key=args.api_key
    )

    # ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system.initialize_all(
        use_gpt4o=args.use_gpt4o,
        openai_api_key=args.api_key
    )

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "Gmail APIë¡œ ì²¨ë¶€íŒŒì¼ ìˆëŠ” ì´ë©”ì¼ ë³´ë‚´ëŠ” ë°©ë²• ì•Œë ¤ì¤˜",
        "Google Driveì—ì„œ ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼ ì°¾ê¸°",
        "Calendar APIë¡œ ì°¸ì„ì ì´ˆëŒ€í•˜ëŠ” ë°©ë²•",
    ]

    print("\n" + "ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œì‘ " + "=" * 40)

    for query in test_queries:
        response = rag_system.search_api(query)
        rag_system.add_conversation_history(query, response)
        input("\në‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    # ëŒ€í™”í˜• ëª¨ë“œ
    print("\n" + "ğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ " + "=" * 40)
    print("ì¢…ë£Œ: 'quit', 'exit', 'ì¢…ë£Œ' ì…ë ¥")
    print("ëª¨ë¸ ì „í™˜: 'switch model' ì…ë ¥")
    print("=" * 60)

    while True:
        user_query = input("\nâ“ ì§ˆë¬¸: ")

        if user_query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if user_query.lower() == 'switch model':
            # ëª¨ë¸ ì „í™˜
            if rag_system.current_model == "qwen":
                if args.api_key:
                    rag_system.initialize_models(use_gpt4o=True, openai_api_key=args.api_key)
                    print("âœ… GPT-4oë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. --api-key ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            else:
                rag_system.initialize_models(use_gpt4o=False)
                print("âœ… Qwen3:8Bë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
            continue

        response = rag_system.search_api(user_query)
        rag_system.add_conversation_history(user_query, response)