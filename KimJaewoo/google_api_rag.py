import os
import json
import torch
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Literal
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import openai


class GoogleAPIRAGSystem:
    """êµ¬ê¸€ API ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ RAG ì‹œìŠ¤í…œ (GPT-4o & Qwen3:8B)"""

    def __init__(self,
                 api_data_dir: str = "./GOOGLE_API_DATA",
                 api_qa_dir: str = "./GOOGLE_API_DATA/GOOGLE_API_DATA_QA",
                 db_dir: str = "./chroma_google_api_db",
                 openai_api_key: Optional[str] = None):
        """
        Args:
            api_data_dir: êµ¬ê¸€ API ì›ë³¸ ë°ì´í„° ë””ë ‰í† ë¦¬
            api_qa_dir: êµ¬ê¸€ API QA ë°ì´í„° ë””ë ‰í† ë¦¬
            db_dir: Chroma DB ì €ì¥ ê²½ë¡œ
            openai_api_key: OpenAI API í‚¤ (GPT-4o ì‚¬ìš©ì‹œ)
        """
        self.api_data_dir = Path(api_data_dir)
        self.api_qa_dir = Path(api_qa_dir)
        self.db_dir = db_dir

        # OpenAI ì„¤ì • (GPT-4oìš©)
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
            openai.api_key = openai_api_key

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.documents = []
        self.vectorstore = None
        self.retriever = None

        # ëª¨ë¸ ê´€ë ¨
        self.qwen_model = None
        self.qwen_tokenizer = None
        self.gpt4o_model = None
        self.embedding_model = None

        # í˜„ì¬ ì‚¬ìš© ëª¨ë¸
        self.current_model: Literal["gpt4o", "qwen"] = "qwen"

    def load_api_documents(self) -> List[Document]:
        """êµ¬ê¸€ API ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  Document ê°ì²´ë¡œ ë³€í™˜"""
        documents = []

        # 1. GOOGLE_API_DATA_QA í´ë”ì—ì„œ QA í˜•ì‹ ë°ì´í„° ë¡œë“œ
        if self.api_qa_dir.exists():
            print(f"ğŸ“‚ QA ë°ì´í„° ë¡œë“œ ì¤‘: {self.api_qa_dir}")

            # JSON íŒŒì¼ ë¡œë“œ
            for file_path in self.api_qa_dir.glob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        qa_data = json.load(f)

                    # QA ìŒì„ ë¬¸ì„œë¡œ ë³€í™˜
                    if isinstance(qa_data, list):
                        for item in qa_data:
                            doc = Document(
                                page_content=f"ì§ˆë¬¸: {item.get('question', '')}\në‹µë³€: {item.get('answer', '')}",
                                metadata={
                                    'type': 'qa',
                                    'question': item.get('question', ''),
                                    'answer': item.get('answer', ''),
                                    'api_category': self._extract_api_category_from_content(item.get('question', '')),
                                    'source_file': file_path.name
                                }
                            )
                            documents.append(doc)

                except Exception as e:
                    print(f"âš ï¸ Error loading {file_path}: {e}")

            # TXT íŒŒì¼ ë¡œë“œ
            for file_path in self.api_qa_dir.glob("*.txt"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # Q&A í˜•ì‹ íŒŒì‹± ì‹œë„
                    if "Q:" in content or "ì§ˆë¬¸:" in content:
                        qa_pairs = self._parse_qa_format(content)
                        for q, a in qa_pairs:
                            doc = Document(
                                page_content=f"ì§ˆë¬¸: {q}\në‹µë³€: {a}",
                                metadata={
                                    'type': 'qa',
                                    'question': q,
                                    'answer': a,
                                    'source_file': file_path.name
                                }
                            )
                            documents.append(doc)
                    else:
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        doc = Document(
                            page_content=content,
                            metadata={
                                'type': 'text',
                                'source_file': file_path.name
                            }
                        )
                        documents.append(doc)

                except Exception as e:
                    print(f"âš ï¸ Error loading {file_path}: {e}")

        # 2. GOOGLE_API_DATA í´ë”ì—ì„œ ì›ë³¸ API ë¬¸ì„œ ë¡œë“œ
        if self.api_data_dir.exists():
            print(f"ğŸ“‚ ì›ë³¸ API ë°ì´í„° ë¡œë“œ ì¤‘: {self.api_data_dir}")

            # í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì²­í‚¹í•˜ì—¬ ë¡œë“œ
            for file_path in self.api_data_dir.glob("*.txt"):
                if file_path.parent == self.api_data_dir:  # í•˜ìœ„ í´ë” ì œì™¸
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # í…ìŠ¤íŠ¸ ì²­í‚¹
                        text_splitter = RecursiveCharacterTextSplitter(
                            chunk_size=1500,
                            chunk_overlap=300,
                            separators=["\n\n\n", "\n\n", "\n", ".", " "]
                        )

                        chunks = text_splitter.split_text(content)

                        for i, chunk in enumerate(chunks):
                            doc = Document(
                                page_content=chunk,
                                metadata={
                                    'type': 'api_doc',
                                    'source_file': file_path.name,
                                    'chunk_id': i,
                                    'api_category': self._extract_api_category(file_path.name)
                                }
                            )
                            documents.append(doc)

                    except Exception as e:
                        print(f"âš ï¸ Error loading {file_path}: {e}")

        # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°)
        if not documents:
            documents = self._create_sample_documents()

        self.documents = documents
        print(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents

    def _parse_qa_format(self, content: str) -> List[Tuple[str, str]]:
        """Q&A í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±"""
        qa_pairs = []

        # ë‹¤ì–‘í•œ Q&A íŒ¨í„´ ì²˜ë¦¬
        import re

        # íŒ¨í„´ 1: Q: ... A: ...
        pattern1 = r'Q[:\.]?\s*(.*?)\s*A[:\.]?\s*(.*?)(?=Q[:\.]?|\Z)'
        matches1 = re.findall(pattern1, content, re.DOTALL)

        # íŒ¨í„´ 2: ì§ˆë¬¸: ... ë‹µë³€: ...
        pattern2 = r'ì§ˆë¬¸[:\.]?\s*(.*?)\s*ë‹µë³€[:\.]?\s*(.*?)(?=ì§ˆë¬¸[:\.]?|\Z)'
        matches2 = re.findall(pattern2, content, re.DOTALL)

        qa_pairs.extend(matches1)
        qa_pairs.extend(matches2)

        # ì •ë¦¬
        qa_pairs = [(q.strip(), a.strip()) for q, a in qa_pairs if q.strip() and a.strip()]

        return qa_pairs

    def _extract_api_category(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ API ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        filename_lower = filename.lower()

        # êµ¬ê¸€ API ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        api_categories = {
            'gmail': 'gmail',
            'drive': 'drive',
            'calendar': 'calendar',
            'sheets': 'sheets',
            'docs': 'docs',
            'slides': 'slides',
            'meet': 'meet',
            'maps': 'maps',
            'youtube': 'youtube',
            'analytics': 'analytics'
        }

        for key in api_categories:
            if key in filename_lower:
                return api_categories[key]

        return 'general'

    def _extract_api_category_from_content(self, content: str) -> str:
        """ë‚´ìš©ì—ì„œ API ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        content_lower = content.lower()

        api_keywords = {
            'gmail': ['gmail', 'ì´ë©”ì¼', 'email', 'messages.send'],
            'drive': ['drive', 'ë“œë¼ì´ë¸Œ', 'files.list', 'files.create'],
            'calendar': ['calendar', 'ìº˜ë¦°ë”', 'events.insert', 'events.list'],
            'sheets': ['sheets', 'ìŠ¤í”„ë ˆë“œì‹œíŠ¸', 'spreadsheet', 'values.update'],
            'docs': ['docs', 'ë¬¸ì„œ', 'documents.create'],
            'youtube': ['youtube', 'ìœ íŠœë¸Œ', 'videos.list'],
            'maps': ['maps', 'ì§€ë„', 'geocoding', 'directions']
        }

        for category, keywords in api_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                return category

        return 'general'

    def _create_sample_documents(self) -> List[Document]:
        """ìƒ˜í”Œ êµ¬ê¸€ API QA ë°ì´í„° ìƒì„±"""
        sample_qa_data = [
            {
                "question": "Gmail APIë¡œ ì´ë©”ì¼ì„ ë³´ë‚´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
                "answer": """Gmail APIë¡œ ì´ë©”ì¼ì„ ë³´ë‚´ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

1. OAuth 2.0 ì¸ì¦ ì„¤ì • (gmail.send ìŠ¤ì½”í”„ í•„ìš”)
2. MIME ë©”ì‹œì§€ ìƒì„±
3. base64url ì¸ì½”ë”©
4. messages.send() ë©”ì„œë“œ í˜¸ì¶œ

ì˜ˆì œ ì½”ë“œ:
```python
from googleapiclient.discovery import build
from email.mime.text import MIMEText
import base64

# ë©”ì‹œì§€ ìƒì„±
message = MIMEText('ì•ˆë…•í•˜ì„¸ìš”!')
message['to'] = 'recipient@example.com'
message['subject'] = 'í…ŒìŠ¤íŠ¸ ì´ë©”ì¼'

# base64 ì¸ì½”ë”©
raw = base64.urlsafe_b64encode(message.as_bytes()).decode()

# ì „ì†¡
service.users().messages().send(userId='me', body={'raw': raw}).execute()
```"""
            },
            {
                "question": "Google Driveì—ì„œ íŠ¹ì • íŒŒì¼ íƒ€ì…ë§Œ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì€?",
                "answer": """Drive APIì˜ files.list() ë©”ì„œë“œì—ì„œ q íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

1. PDF íŒŒì¼ë§Œ: q="mimeType='application/pdf'"
2. ì´ë¯¸ì§€ íŒŒì¼: q="mimeType contains 'image/'"
3. íŠ¹ì • í´ë” ë‚´: q="'FOLDER_ID' in parents"
4. ì´ë¦„ í¬í•¨: q="name contains 'report'"

ì˜ˆì œ:
```python
results = service.files().list(
    q="mimeType='application/pdf' and name contains '2024'",
    pageSize=10,
    fields="files(id, name, mimeType)"
).execute()
```"""
            },
            {
                "question": "Calendar APIë¡œ ë°˜ë³µ ì´ë²¤íŠ¸ë¥¼ ë§Œë“¤ë ¤ë©´?",
                "answer": """ë°˜ë³µ ì´ë²¤íŠ¸ëŠ” recurrence í•„ë“œë¥¼ ì‚¬ìš©í•´ìš”:

```python
event = {
    'summary': 'ì£¼ê°„ íšŒì˜',
    'start': {'dateTime': '2024-01-15T10:00:00', 'timeZone': 'Asia/Seoul'},
    'end': {'dateTime': '2024-01-15T11:00:00', 'timeZone': 'Asia/Seoul'},
    'recurrence': [
        'RRULE:FREQ=WEEKLY;COUNT=10'  # 10ì£¼ê°„ ë§¤ì£¼ ë°˜ë³µ
    ]
}

service.events().insert(calendarId='primary', body=event).execute()
```

RRULE ì˜ˆì‹œ:
- ë§¤ì¼: FREQ=DAILY
- ë§¤ì£¼ ì›”,ìˆ˜,ê¸ˆ: FREQ=WEEKLY;BYDAY=MO,WE,FR
- ë§¤ì›” 15ì¼: FREQ=MONTHLY;BYMONTHDAY=15"""
            }
        ]

        documents = []
        for item in sample_qa_data:
            doc = Document(
                page_content=f"ì§ˆë¬¸: {item['question']}\në‹µë³€: {item['answer']}",
                metadata={
                    'type': 'qa',
                    'question': item['question'],
                    'answer': item['answer'],
                    'api_category': self._extract_api_category_from_content(item['question'])
                }
            )
            documents.append(doc)

        return documents

    def initialize_vectorstore(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì„ë² ë”©"""
        print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        print("ğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        self.vectorstore = Chroma.from_documents(
            documents=self.documents,
            embedding=self.embedding_model,
            persist_directory=self.db_dir,
            collection_metadata={"hnsw:space": "cosine"}
        )

        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 5,
                "score_threshold": 0.3
            }
        )

        print(f"âœ… ë²¡í„° ì €ì¥ì†Œê°€ {self.db_dir}ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def initialize_models(self, use_gpt4o: bool = False, openai_api_key: Optional[str] = None):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""

        if use_gpt4o:
            # GPT-4o ì´ˆê¸°í™”
            if openai_api_key:
                os.environ["OPENAI_API_KEY"] = openai_api_key

            print("ğŸ¤– GPT-4o ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            self.gpt4o_model = ChatOpenAI(
                model="gpt-4o",
                temperature=0.7,
                max_tokens=1024
            )
            self.current_model = "gpt4o"
            print("âœ… GPT-4o ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

        else:
            # Qwen3:8B ì´ˆê¸°í™” (ë¡œì»¬ ëª¨ë¸)
            print("ğŸ¤– Qwen3:8B ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")

            model_name = "Qwen/Qwen2.5-7B-Instruct"  # ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ì—…ê·¸ë ˆì´ë“œ
            local_model_path = "./Qwen2.5-7B-Instruct"

            if os.path.exists(local_model_path):
                print(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘: {local_model_path}")
                self.qwen_tokenizer = AutoTokenizer.from_pretrained(
                    local_model_path,
                    local_files_only=True
                )
                self.qwen_model = AutoModelForCausalLM.from_pretrained(
                    local_model_path,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    local_files_only=True
                )
            else:
                print(f"â¬‡ï¸ Hugging Faceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}")
                self.qwen_tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.qwen_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                # ë¡œì»¬ ì €ì¥
                self.qwen_tokenizer.save_pretrained(local_model_path)
                self.qwen_model.save_pretrained(local_model_path)
                print(f"ğŸ’¾ ëª¨ë¸ì´ {local_model_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            self.current_model = "qwen"
            print("âœ… Qwen3:8B ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")

    def format_docs_for_context(self, docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        formatted = []

        for i, doc in enumerate(docs, 1):
            if doc.metadata.get('type') == 'qa':
                # QA í˜•ì‹ ë¬¸ì„œ
                formatted.append(f"[ì°¸ê³  {i}]\n{doc.page_content}")
            else:
                # ì¼ë°˜ ë¬¸ì„œ
                content = f"[ì°¸ê³  {i}]\n"
                content += f"ë‚´ìš©: {doc.page_content[:500]}..."  # ê¸¸ì´ ì œí•œ
                if 'api_category' in doc.metadata:
                    content += f"\nì¹´í…Œê³ ë¦¬: {doc.metadata['api_category']}"
                formatted.append(content)

        return "\n\n---\n\n".join(formatted)

    def get_prompt_template(self, use_haeyoche: bool = False) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""

        if use_haeyoche:
            # Qwen ëª¨ë¸ìš© (í•´ìš”ì²´)
            template = """ë‹¹ì‹ ì€ êµ¬ê¸€ API ì „ë¬¸ê°€ì˜ˆìš”. ë‹¤ìŒì˜ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë°œìì—ê²Œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€í•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ì •í™•í•œ API ë©”ì„œë“œëª…ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”
2. ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
3. í•„ìš”í•œ ê¶Œí•œì´ë‚˜ ì£¼ì˜ì‚¬í•­ì´ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
4. ì¹œê·¼í•œ í•´ìš”ì²´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”

ë‹µë³€:"""
        else:
            # GPT-4oìš© (ë©€í‹°í„´ ê³ ë ¤)
            template = """You are a Google API expert. Based on the following reference materials, provide accurate and practical answers to help developers.

Reference Materials:
{context}

Question: {question}

Please ensure your answer includes:
1. Exact API method names and parameters
2. Practical code examples
3. Required permissions or important notes
4. Clear and structured explanation

Answer in Korean:"""

        return template

    def retrieve_with_scores(self, query: str, k: int = 5) -> Tuple[List[Document], List[float]]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)"""
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        docs = [doc for doc, _ in results]
        scores = [score for _, score in results]
        return docs, scores

    def generate_response_with_gpt4o(self, query: str, context: str) -> str:
        """GPT-4oë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (ë©€í‹°í„´ ì§€ì›)"""
        messages = [
            SystemMessage(content="""ë‹¹ì‹ ì€ êµ¬ê¸€ API ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ê°œë°œìë“¤ì—ê²Œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            ì½”ë“œ ì˜ˆì‹œì™€ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""),
            HumanMessage(content=f"""
ì°¸ê³  ìë£Œ:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€í•´ì£¼ì„¸ìš”:""")
        ]

        response = self.gpt4o_model.invoke(messages)
        return response.content

    def generate_response_with_qwen(self, query: str, context: str) -> str:
        """Qwen ëª¨ë¸ì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (í•´ìš”ì²´)"""
        prompt_template = self.get_prompt_template(use_haeyoche=True)
        prompt_text = prompt_template.format(context=context, question=query)

        # í† í°í™”
        inputs = self.qwen_tokenizer(
            prompt_text,
            return_tensors="pt",
            max_length=2048,
            truncation=True
        ).to(self.qwen_model.device)

        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = self.qwen_model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.qwen_tokenizer.eos_token_id,
                eos_token_id=self.qwen_tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        full_response = self.qwen_tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer_only = full_response.replace(prompt_text, "").strip()

        return answer_only

    def generate_response(self, query: str, use_gpt4o: Optional[bool] = None) -> Tuple[
        List[Document], List[float], str]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""

        # ëª¨ë¸ ì„ íƒ
        if use_gpt4o is None:
            use_gpt4o = (self.current_model == "gpt4o")

        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        docs, scores = self.retrieve_with_scores(query)

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = self.format_docs_for_context(docs)

        # ì‘ë‹µ ìƒì„±
        if use_gpt4o and self.gpt4o_model:
            response = self.generate_response_with_gpt4o(query, context)
        elif self.qwen_model:
            response = self.generate_response_with_qwen(query, context)
        else:
            response = "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize_models()ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."

        return docs, scores, response

    def initialize_all(self, use_gpt4o: bool = False, openai_api_key: Optional[str] = None):
        """ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("=" * 60)
        print("ğŸš€ Google API RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        print("=" * 60)

        # 1. ë¬¸ì„œ ë¡œë“œ
        print("\nğŸ“š [1/3] API ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        self.load_api_documents()

        # 2. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
        print("\nğŸ” [2/3] ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        self.initialize_vectorstore()

        # 3. LLM ì´ˆê¸°í™”
        print("\nğŸ¤– [3/3] LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.initialize_models(use_gpt4o=use_gpt4o, openai_api_key=openai_api_key)

        print("\n" + "=" * 60)
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"ğŸ“Š í˜„ì¬ ëª¨ë¸: {'GPT-4o' if self.current_model == 'gpt4o' else 'Qwen3:8B'}")
        print("=" * 60 + "\n")

    def search_api(self, query: str, use_gpt4o: Optional[bool] = None, verbose: bool = True):
        """API ê²€ìƒ‰ ë° ì‘ë‹µ ì œê³µ"""
        docs, scores, response = self.generate_response(query, use_gpt4o)

        if verbose:
            print("\n" + "=" * 60)
            print(f"ğŸ” ì§ˆë¬¸: {query}")
            print(f"ğŸ¤– ëª¨ë¸: {'GPT-4o' if (use_gpt4o or self.current_model == 'gpt4o') else 'Qwen3:8B (í•´ìš”ì²´)'}")
            print("=" * 60)

            print("\nğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:")
            for i, (doc, score) in enumerate(zip(docs[:3], scores[:3]), 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                doc_type = doc.metadata.get('type', 'unknown')
                if doc_type == 'qa':
                    print(f"\n  [{i}] QA ë¬¸ì„œ (ìœ ì‚¬ë„: {score:.4f})")
                    print(f"      ì§ˆë¬¸: {doc.metadata.get('question', 'N/A')[:50]}...")
                else:
                    category = doc.metadata.get('api_category', 'general')
                    print(f"\n  [{i}] API ë¬¸ì„œ - {category} (ìœ ì‚¬ë„: {score:.4f})")
                    print(f"      ë‚´ìš©: {doc.page_content[:80]}...")

            print("\n" + "-" * 60)
            print("ğŸ’¡ ë‹µë³€:")
            print("-" * 60)
            print(response)
            print("=" * 60 + "\n")

        return response

    def add_conversation_history(self, query: str, response: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ë©€í‹°í„´ ì§€ì›)"""
        # í–¥í›„ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬ ì €ì¥
        if not hasattr(self, 'conversation_history'):
            self.conversation_history = []

        self.conversation_history.append({
            'query': query,
            'response': response,
            'model': self.current_model
        })


# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    import argparse

    # ëª…ë ¹ì¤„ ì¸ì íŒŒì„œ
    parser = argparse.ArgumentParser(description='Google API RAG System')
    parser.add_argument('--use-gpt4o', action='store_true', help='Use GPT-4o instead of Qwen')
    parser.add_argument('--api-key', type=str, help='OpenAI API key for GPT-4o')
    args = parser.parse_args()

    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = GoogleAPIRAGSystem(
        api_data_dir="../GOOGLE_API_DATA",
        api_qa_dir="../GOOGLE_API_DATA/GOOGLE_API_DATA_QA",
        db_dir="../chroma_google_api_db",
        openai_api_key=args.api_key
    )

    # ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system.initialize_all(
        use_gpt4o=args.use_gpt4o,
        openai_api_key=args.api_key
    )

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "Gmail APIë¡œ ì²¨ë¶€íŒŒì¼ ìˆëŠ” ì´ë©”ì¼ ë³´ë‚´ëŠ” ë°©ë²• ì•Œë ¤ì¤˜",
        "Google Driveì—ì„œ ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼ ì°¾ê¸°",
        "Calendar APIë¡œ ì°¸ì„ì ì´ˆëŒ€í•˜ëŠ” ë°©ë²•",
    ]

    print("\n" + "ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œì‘ " + "=" * 40)

    for query in test_queries:
        response = rag_system.search_api(query)
        rag_system.add_conversation_history(query, response)
        input("\në‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

    # ëŒ€í™”í˜• ëª¨ë“œ
    print("\n" + "ğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ " + "=" * 40)
    print("ì¢…ë£Œ: 'quit', 'exit', 'ì¢…ë£Œ' ì…ë ¥")
    print("ëª¨ë¸ ì „í™˜: 'switch model' ì…ë ¥")
    print("=" * 60)

    while True:
        user_query = input("\nâ“ ì§ˆë¬¸: ")

        if user_query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if user_query.lower() == 'switch model':
            # ëª¨ë¸ ì „í™˜
            if rag_system.current_model == "qwen":
                if args.api_key:
                    rag_system.initialize_models(use_gpt4o=True, openai_api_key=args.api_key)
                    print("âœ… GPT-4oë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. --api-key ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            else:
                rag_system.initialize_models(use_gpt4o=False)
                print("âœ… Qwen3:8Bë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
            continue

        response = rag_system.search_api(user_query)
        rag_system.add_conversation_history(user_query, response)