import os
import re
import torch
from pathlib import Path
from typing import List, Optional
from tqdm import tqdm
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from concurrent.futures import ThreadPoolExecutor, as_completed


class GoogleAPIDocumentProcessor:
    def __init__(self,
                 api_data_dir: str = "./GOOGLE_API_DATA",
                 db_dir: str = "./chroma_google_api_db"):
        self.api_data_dir = Path(api_data_dir)
        self.db_dir = db_dir
        self.documents: List[Document] = []
        self.vectorstore: Optional[Chroma] = None
        self.embedding_model: Optional[HuggingFaceEmbeddings] = None

    def get_api_tag_from_path(self, path: str) -> str:
        folder = os.path.basename(os.path.dirname(path))
        if folder.endswith("_docs_crawled"):
            return folder.replace("_docs_crawled", "")
        return folder

    def _extract_source_url(self, content: str) -> str:
        pattern = r'(?i)Source\s*URL\s*:\s*(https?://\S+)'
        m = re.search(pattern, content)
        if m:
            return m.group(1).strip()
        head = content[:2048]
        m2 = re.search(pattern, head)
        return m2.group(1).strip() if m2 else ""

    def load_api_documents(self) -> List[Document]:
        documents = []

        if not self.api_data_dir.exists():
            print(f"âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.api_data_dir}")
            return documents

        print(f"ğŸ“‚ API ë°ì´í„° ë¡œë“œ ì¤‘ (.txt íŒŒì¼ë§Œ íƒìƒ‰): {self.api_data_dir}")
        file_paths = list(self.api_data_dir.rglob("*.txt"))

        for file_path in file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                source_url = self._extract_source_url(content)
                tag = self.get_api_tag_from_path(str(file_path))  # str() ì¶”ê°€

                if tag is None:
                    continue

                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1200,
                    chunk_overlap=150,
                    separators=["\n\n", "\n", ". ", " ", ""]
                )
                chunks = text_splitter.split_text(content)

                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            'chunk_id': i,
                            'source': source_url,
                            'tags': tag,
                            'source_file': file_path.name,
                            'last_verified': '2025-08-19'
                        }
                    )
                    documents.append(doc)

            except Exception as e:
                print(f"âš ï¸ {file_path} íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        self.documents = documents
        print(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return documents

    def initialize_vectorstore_parallel(self, batch_size: int = 100, max_workers: int = 4):
        if not self.documents:
            print("âš ï¸ ë²¡í„° DBë¥¼ ìƒì„±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. `load_api_documents`ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return

        print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... (BAAI/bge-m3)")

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ“± ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

        self.embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )

        if os.path.exists(self.db_dir) and any(Path(self.db_dir).iterdir()):
            print(f"ğŸ’¾ ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œê°€ '{self.db_dir}'ì— ì¡´ì¬í•©ë‹ˆë‹¤.")
            user_input = input("ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
            if user_input != 'y':
                print("ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
            else:
                import shutil
                shutil.rmtree(self.db_dir)
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ '{self.db_dir}' í´ë”ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

        print("ğŸ’¾ ìƒˆ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")

        # ì²« ë°°ì¹˜ë¡œ DB ìƒì„±
        first_batch = self.documents[:batch_size]
        self.vectorstore = Chroma.from_documents(
            documents=first_batch,
            embedding=self.embedding_model,
            persist_directory=self.db_dir,
            collection_name="google_api_docs"  # ì»¬ë ‰ì…˜ ì´ë¦„ ëª…ì‹œ
        )

        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì²˜ë¦¬
        remaining_docs = self.documents[batch_size:]
        if remaining_docs:
            batches = [remaining_docs[i:i + batch_size] for i in range(0, len(remaining_docs), batch_size)]

            for batch in tqdm(batches, desc="ì„ë² ë”© ë° DB ì €ì¥ ì¤‘"):
                self.vectorstore.add_documents(batch)

        # ëª…ì‹œì ìœ¼ë¡œ persist í˜¸ì¶œ (ì¤‘ìš”!)
        self.vectorstore.persist()

        print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ ({self.db_dir})")
        print(f"ğŸ“Š ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {self.vectorstore._collection.count()}")

    def verify_db(self):
        """DBê°€ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if os.path.exists(self.db_dir):
            print(f"\nğŸ” DB ê²€ì¦ ì¤‘...")

            # ì„ë² ë”© ëª¨ë¸ ì¬ì´ˆê¸°í™”
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            embedding_model = HuggingFaceEmbeddings(
                model_name="BAAI/bge-m3",
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )

            # DB ë¡œë“œ
            loaded_db = Chroma(
                persist_directory=self.db_dir,
                embedding_function=embedding_model,
                collection_name="google_api_docs"
            )

            doc_count = loaded_db._collection.count()
            print(f"ğŸ“š DBì— ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {doc_count}")

            # ìƒ˜í”Œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            if doc_count > 0:
                results = loaded_db.similarity_search("Google API", k=3)
                print(f"ğŸ” ìƒ˜í”Œ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
                for i, doc in enumerate(results[:2], 1):
                    print(f"\n  [{i}] {doc.metadata.get('source_file', 'Unknown')}")
                    print(f"      Tag: {doc.metadata.get('tags', 'Unknown')}")
                    print(f"      ë‚´ìš© ì¼ë¶€: {doc.page_content[:100]}...")

            return doc_count > 0
        else:
            print(f"âŒ DB ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_dir}")
            return False


if __name__ == "__main__":
    try:
        processor = GoogleAPIDocumentProcessor(
            api_data_dir='../GOOGLE_API_DATA',
            db_dir='./chroma_google_api_db'
        )

        print("=" * 60)
        print("ğŸš€ API ë¬¸ì„œ ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
        print("=" * 60)

        processor.load_api_documents()
        processor.initialize_vectorstore_parallel()

        # DB ê²€ì¦
        processor.verify_db()

        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 60 + "\n")

    except Exception as e:
        print(f"ğŸ’¥ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")