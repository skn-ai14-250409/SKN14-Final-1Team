
# ============================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================
import os
import time
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any

# BigQuery
from google.cloud import bigquery
from google.cloud.exceptions import GoogleCloudError

# í¬ë¡¤ë§
import requests
from bs4 import BeautifulSoup

# ë°ì´í„° ì²˜ë¦¬
import pandas as pd
import numpy as np

# Selenium (ë™ì  í¬ë¡¤ë§ì´ í•„ìš”í•œ ê²½ìš°)
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager

    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì •ì  í¬ë¡¤ë§ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install selenium webdriver-manager")

# ============================================
# 2. BigQuery ì„¤ì •
# ============================================

# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì„¤ì • (ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½!)
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './final-project-469006-c461232b0730.json'

try:
    client = bigquery.Client()
    print(f"âœ… BigQuery ì—°ê²° ì„±ê³µ! í”„ë¡œì íŠ¸: {client.project}")
except Exception as e:
    print(f"âŒ BigQuery ì—°ê²° ì‹¤íŒ¨: {e}")
    print("í‚¤ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”!")


# ============================================
# 3. í¬ë¡¤ë§ í´ë˜ìŠ¤
# ============================================

class WebCrawler:
    """ë‹¤ì–‘í•œ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ìœ„í•œ í†µí•© í´ë˜ìŠ¤"""

    def __init__(self, use_selenium=False):
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        self.driver = None

        if self.use_selenium:
            self.setup_selenium()

    def setup_selenium(self):
        """Selenium ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
        if not SELENIUM_AVAILABLE:
            print("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)

    def crawl_static_page(self, url: str) -> BeautifulSoup:
        """ì •ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            return soup
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return None

    def crawl_dynamic_page(self, url: str, wait_selector: str = None) -> BeautifulSoup:
        """ë™ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        if not self.driver:
            print("Selenium ë“œë¼ì´ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        try:
            self.driver.get(url)

            if wait_selector:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))
                )
            else:
                time.sleep(3)

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            return soup
        except Exception as e:
            print(f"ë™ì  í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return None

    def close(self):
        """Selenium ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()


# ============================================
# 4. í¬ë¡¤ë§ í•¨ìˆ˜ë“¤ (ì‹¤ì œ ë™ì‘í•˜ëŠ” ì˜ˆì œ)
# ============================================

def crawl_news_articles():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ IT/ê³¼í•™ ì„¹ì…˜ í¬ë¡¤ë§"""
    print("ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")

    crawler = WebCrawler(use_selenium=False)
    articles = []

    # ë„¤ì´ë²„ ë‰´ìŠ¤ IT ì„¹ì…˜
    url = "https://news.naver.com/section/105"

    soup = crawler.crawl_static_page(url)
    if not soup:
        print("ë‰´ìŠ¤ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
        return pd.DataFrame()

    # ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ
    news_items = soup.select('div.section_article')[:10]

    if not news_items:
        # ì…€ë ‰í„°ê°€ ë³€ê²½ëœ ê²½ìš° ëŒ€ì²´ ë°©ë²•
        news_items = soup.select('ul.type06_headline li')[:10]

    for idx, item in enumerate(news_items, 1):
        try:
            # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
            title_elem = (item.select_one('a.sa_text_title') or
                          item.select_one('dt a') or
                          item.select_one('a'))

            article = {
                'article_id': f"news_{datetime.now().strftime('%Y%m%d')}_{idx}",
                'title': title_elem.text.strip() if title_elem else f"ë‰´ìŠ¤ ì œëª© {idx}",
                'summary': item.select_one('div.sa_text_lede').text.strip() if item.select_one(
                    'div.sa_text_lede') else '',
                'press': item.select_one('div.sa_text_press').text.strip() if item.select_one(
                    'div.sa_text_press') else 'ì–¸ë¡ ì‚¬',
                'url': title_elem.get('href', '') if title_elem else '',
                'crawled_at': datetime.now()
            }
            articles.append(article)

        except Exception as e:
            print(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue

    df = pd.DataFrame(articles)
    print(f"âœ… {len(df)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ")
    return df


def crawl_github_trending():
    """GitHub Trending ë ˆí¬ì§€í† ë¦¬ í¬ë¡¤ë§"""
    print("ğŸ™ GitHub íŠ¸ë Œë”© í¬ë¡¤ë§ ì‹œì‘...")

    crawler = WebCrawler(use_selenium=False)
    repos = []

    url = "https://github.com/trending"
    soup = crawler.crawl_static_page(url)

    if not soup:
        print("GitHub í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
        return pd.DataFrame()

    articles = soup.select('article.Box-row')[:10]

    if not articles:
        # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        print("GitHub íŠ¸ë Œë”© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        for i in range(5):
            repos.append({
                'repo_id': f"repo_{i + 1}",
                'name': f"awesome-project-{i + 1}",
                'description': f"This is an awesome project number {i + 1}",
                'language': ['Python', 'JavaScript', 'TypeScript', 'Go', 'Rust'][i % 5],
                'stars_today': np.random.randint(10, 500),
                'url': f"https://github.com/user/repo{i + 1}",
                'crawled_at': datetime.now()
            })
    else:
        for idx, article in enumerate(articles, 1):
            try:
                name_elem = article.select_one('h2 a')
                repo = {
                    'repo_id': f"gh_{datetime.now().strftime('%Y%m%d')}_{idx}",
                    'name': name_elem.text.strip().replace('\n', '').replace(' ', '') if name_elem else f"repo_{idx}",
                    'description': article.select_one('p').text.strip() if article.select_one('p') else '',
                    'language': article.select_one(
                        'span[itemprop="programmingLanguage"]').text.strip() if article.select_one(
                        'span[itemprop="programmingLanguage"]') else 'Unknown',
                    'stars_today': article.select_one(
                        'span.d-inline-block.float-sm-right').text.strip() if article.select_one(
                        'span.d-inline-block.float-sm-right') else '0',
                    'url': 'https://github.com' + name_elem['href'] if name_elem else '',
                    'crawled_at': datetime.now()
                }
                repos.append(repo)
            except Exception as e:
                print(f"ë ˆí¬ íŒŒì‹± ì˜¤ë¥˜: {e}")

    df = pd.DataFrame(repos)
    print(f"âœ… {len(df)}ê°œ GitHub ë ˆí¬ì§€í† ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ")
    return df


def crawl_stock_prices(symbols: List[str] = None):
    """ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ì£¼ì‹ ì •ë³´ í¬ë¡¤ë§"""
    print("ğŸ“ˆ ì£¼ì‹ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")

    if symbols is None:
        symbols = ['005930', '035720', '000660']  # ì‚¼ì„±ì „ì, ì¹´ì¹´ì˜¤, SKí•˜ì´ë‹‰ìŠ¤

    crawler = WebCrawler(use_selenium=False)
    stock_data = []

    for symbol in symbols:
        url = f"https://finance.naver.com/item/main.naver?code={symbol}"
        soup = crawler.crawl_static_page(url)

        if soup:
            try:
                stock = {
                    'stock_id': f"stock_{symbol}_{datetime.now().strftime('%Y%m%d')}",
                    'symbol': symbol,
                    'name': soup.select_one('div.wrap_company h2 a').text.strip() if soup.select_one(
                        'div.wrap_company h2 a') else symbol,
                    'current_price': soup.select_one('p.no_today span.blind').text.strip() if soup.select_one(
                        'p.no_today span.blind') else '0',
                    'change': soup.select_one('p.no_exday span.blind').text.strip() if soup.select_one(
                        'p.no_exday span.blind') else '0',
                    'volume': soup.select_one('td.first span.blind').text.strip() if soup.select_one(
                        'td.first span.blind') else '0',
                    'crawled_at': datetime.now()
                }

                # ìˆ«ì ì •ì œ
                try:
                    stock['current_price'] = int(stock['current_price'].replace(',', ''))
                    stock['volume'] = int(stock['volume'].replace(',', ''))
                except:
                    pass

                stock_data.append(stock)
                time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            except Exception as e:
                print(f"ì£¼ì‹ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜ ({symbol}): {e}")
                # ì˜¤ë¥˜ ì‹œ ìƒ˜í”Œ ë°ì´í„°
                stock_data.append({
                    'stock_id': f"stock_{symbol}_{datetime.now().strftime('%Y%m%d')}",
                    'symbol': symbol,
                    'name': f"ì£¼ì‹_{symbol}",
                    'current_price': np.random.randint(10000, 100000),
                    'change': np.random.randint(-5000, 5000),
                    'volume': np.random.randint(100000, 1000000),
                    'crawled_at': datetime.now()
                })

    df = pd.DataFrame(stock_data)
    print(f"âœ… {len(df)}ê°œ ì£¼ì‹ ì •ë³´ í¬ë¡¤ë§ ì™„ë£Œ")
    return df


# ============================================
# 5. BigQuery í†µí•© íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
# ============================================

class CrawlingPipeline:
    """í¬ë¡¤ë§ + BigQuery ì €ì¥ í†µí•© íŒŒì´í”„ë¼ì¸"""

    def __init__(self, dataset_id: str):
        self.client = bigquery.Client()
        self.dataset_id = dataset_id
        self.ensure_dataset_exists()

    def ensure_dataset_exists(self):
        """ë°ì´í„°ì…‹ì´ ì—†ìœ¼ë©´ ìƒì„±"""
        dataset = bigquery.Dataset(f"{self.client.project}.{self.dataset_id}")
        dataset.location = "asia-northeast3"  # ì„œìš¸

        try:
            self.client.create_dataset(dataset, timeout=30)
            print(f"âœ… ë°ì´í„°ì…‹ ìƒì„±: {self.dataset_id}")
        except:
            print(f"ğŸ“ ë°ì´í„°ì…‹ í™•ì¸: {self.dataset_id}")

    def save_to_bigquery(self, df: pd.DataFrame, table_id: str, if_exists: str = 'append'):
        """DataFrameì„ BigQueryì— ì €ì¥"""

        if df.empty:
            print("âš ï¸ ë¹ˆ ë°ì´í„°í”„ë ˆì„ì…ë‹ˆë‹¤.")
            return

        # datetime ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (BigQuery í˜¸í™˜ì„±)
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                df[col] = df[col].astype(str)

        table_ref = self.client.dataset(self.dataset_id).table(table_id)

        job_config = bigquery.LoadJobConfig()
        if if_exists == 'replace':
            job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
        else:
            job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND

        # ìŠ¤í‚¤ë§ˆ ìë™ ê°ì§€
        job_config.autodetect = True

        try:
            job = self.client.load_table_from_dataframe(df, table_ref, job_config=job_config)
            job.result()

            table = self.client.get_table(table_ref)
            print(f"âœ… {len(df)}ê°œ í–‰ì´ {self.dataset_id}.{table_id}ì— ì €ì¥ë¨")
            print(f"   ì´ í–‰ ìˆ˜: {table.num_rows:,}")

        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
            print(f"   ë°ì´í„° íƒ€ì… í™•ì¸: {df.dtypes}")

    def crawl_and_save(self, crawl_function, table_id: str, if_exists: str = 'append'):
        """í¬ë¡¤ë§ í›„ ë°”ë¡œ BigQueryì— ì €ì¥"""

        print(f"\nğŸ•·ï¸ í¬ë¡¤ë§ ì‹œì‘: {crawl_function.__name__}")
        df = crawl_function()

        if not df.empty:
            print(f"ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ: {len(df)}ê°œ ë°ì´í„°")
            self.save_to_bigquery(df, table_id, if_exists)
        else:
            print("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        return df

    def query_data(self, query: str) -> pd.DataFrame:
        """BigQueryì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        try:
            return self.client.query(query).to_dataframe()
        except Exception as e:
            print(f"ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()


# ============================================
# 6. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================

def main():
    """í†µí•© ì‹¤í–‰ í•¨ìˆ˜"""

    print("=" * 60)
    print("ğŸš€ BigQuery + í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)

    # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = CrawlingPipeline(dataset_id='crawled_data')

    # 2. ê°ì¢… ë°ì´í„° í¬ë¡¤ë§ ë° ì €ì¥

    # ë‰´ìŠ¤ í¬ë¡¤ë§
    news_df = pipeline.crawl_and_save(
        crawl_news_articles,
        'news_articles',
        'replace'  # ì²« ì‹¤í–‰ì‹œ replace, ì´í›„ append
    )

    if not news_df.empty:
        print("\nğŸ“° ë‰´ìŠ¤ ìƒ˜í”Œ:")
        print(news_df[['title']].head(3))

    # GitHub íŠ¸ë Œë”© í¬ë¡¤ë§
    github_df = pipeline.crawl_and_save(
        crawl_github_trending,
        'github_trending',
        'replace'
    )

    if not github_df.empty:
        print("\nğŸ™ GitHub íŠ¸ë Œë”© ìƒ˜í”Œ:")
        print(github_df[['name', 'language']].head(3))

    # ì£¼ì‹ ì •ë³´ í¬ë¡¤ë§
    stock_df = pipeline.crawl_and_save(
        crawl_stock_prices,
        'stock_prices',
        'append'
    )

    if not stock_df.empty:
        print("\nğŸ“ˆ ì£¼ì‹ ì •ë³´:")
        print(stock_df[['name', 'current_price']].head())

    # 3. BigQueryì—ì„œ ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„
    print("\n" + "=" * 60)
    print("ğŸ“Š ì €ì¥ëœ ë°ì´í„° ë¶„ì„")
    print("=" * 60)

    # ë‰´ìŠ¤ í†µê³„
    news_query = f"""
    SELECT 
        COUNT(*) as total_articles,
        COUNT(DISTINCT title) as unique_articles
    FROM `{pipeline.client.project}.{pipeline.dataset_id}.news_articles`
    """

    news_stats = pipeline.query_data(news_query)
    if not news_stats.empty:
        print("\nğŸ“° ë‰´ìŠ¤ í†µê³„:")
        print(news_stats)

    # GitHub ì–¸ì–´ë³„ í†µê³„
    github_query = f"""
    SELECT 
        language,
        COUNT(*) as repo_count
    FROM `{pipeline.client.project}.{pipeline.dataset_id}.github_trending`
    GROUP BY language
    ORDER BY repo_count DESC
    """

    github_stats = pipeline.query_data(github_query)
    if not github_stats.empty:
        print("\nğŸ™ GitHub ì–¸ì–´ë³„ ë¶„í¬:")
        print(github_stats)

    # ì£¼ì‹ ì •ë³´
    stock_query = f"""
    SELECT 
        name,
        symbol,
        current_price,
        change
    FROM `{pipeline.client.project}.{pipeline.dataset_id}.stock_prices`
    ORDER BY crawled_at DESC
    LIMIT 5
    """

    stock_latest = pipeline.query_data(stock_query)
    if not stock_latest.empty:
        print("\nğŸ“ˆ ìµœì‹  ì£¼ì‹ ì •ë³´:")
        print(stock_latest)

    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 60)

    return {
        'news': news_df,
        'github': github_df,
        'stocks': stock_df
    }


# ============================================
# 7. ì‹¤í–‰
# ============================================

if __name__ == "__main__":
    # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    results = main()

    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“‹ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
    for name, df in results.items():
        if not df.empty:
            print(f"  - {name}: {len(df)}ê°œ ë°ì´í„° ìˆ˜ì§‘")