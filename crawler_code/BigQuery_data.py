import os
import re
import time
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException

# ì‹œì‘ URL
BASE_URL = "https://cloud.google.com"
START_URL = "/bigquery/docs/reference/rest"

# ì €ì¥í•  í´ë” ì´ë¦„
OUTPUT_DIR = "../GOOGLE_API_DATA/bigquery_docs_crawled"

# ê²°ê³¼ ì €ì¥ í´ë” ìƒì„±
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"ğŸ“ '{OUTPUT_DIR}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

# ì…€ë ˆë‹ˆì›€ ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless")  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920,1080")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

# ì›¹ ë“œë¼ì´ë²„ ì„œë¹„ìŠ¤ ì„¤ì • ë° ì‹¤í–‰
print("ğŸš€ Chrome ë“œë¼ì´ë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
service = ChromeService()
driver = webdriver.Chrome(service=service, options=chrome_options)


def clean_filename(url):
    """URLì„ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
    path = url.replace(BASE_URL, "").replace("https://", "").replace("http://", "")
    # íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì¹˜í™˜
    filename = re.sub(r'[/\\?%*:|"<>]', "_", path).strip("_")
    # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    if len(filename) > 200:
        filename = filename[:200]
    return filename + ".txt"


def extract_page_content(driver, url):
    """í˜ì´ì§€ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
        wait = WebDriverWait(driver, 15)

        # main ë˜ëŠ” article íƒœê·¸ ì°¾ê¸° (Google Cloud ë¬¸ì„œ êµ¬ì¡°)
        content_element = None

        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì½˜í…ì¸  ì»¨í…Œì´ë„ˆë¥¼ ì‹œë„
        content_selectors = [
            "article",
            "main",
            "[role='main']",
            ".devsite-article",
            ".devsite-main-content",
            "#gc-wrapper"
        ]

        for selector in content_selectors:
            try:
                content_element = wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                if content_element:
                    print(f"  âœ“ ì½˜í…ì¸  ì˜ì—­ ë°œê²¬: {selector}")
                    break
            except TimeoutException:
                continue

        if not content_element:
            print("  âš ï¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ bodyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            content_element = driver.find_element(By.TAG_NAME, "body")

        # ë§í¬ì— URL ì£¼ì†Œ ì¶”ê°€ (YouTube í¬ë¡¤ëŸ¬ì™€ ë™ì¼í•œ ë°©ì‹)
        try:
            links_in_content = content_element.find_elements(By.TAG_NAME, "a")
            for link in links_in_content:
                href = link.get_attribute("href")
                if href and "javascript:void(0)" not in href and "#" not in href:
                    # JavaScriptë¥¼ ì‚¬ìš©í•´ ë§í¬ í…ìŠ¤íŠ¸ ë’¤ì— URL ì¶”ê°€
                    driver.execute_script(
                        "if (arguments[0].textContent && !arguments[0].textContent.includes('[http')) {"
                        "arguments[0].textContent = arguments[0].textContent.trim() + ' [' + arguments[0].href + ']';"
                        "}",
                        link
                    )
        except Exception as e:
            print(f"  âš ï¸ ë§í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        final_page_text = content_element.text

        # ì½”ë“œ ë¸”ë¡ íŠ¹ë³„ ì²˜ë¦¬ (Google Cloud ë¬¸ì„œëŠ” ì½”ë“œ ì˜ˆì œê°€ ë§ìŒ)
        try:
            code_blocks = content_element.find_elements(By.CSS_SELECTOR,
                                                        "pre, code.devsite-code-highlight, .prettyprint")
            for code_block in code_blocks:
                code_text = code_block.get_attribute("textContent")
                if code_text and len(code_text.strip()) > 0:
                    # ì½”ë“œ ë¸”ë¡ì„ ëª…í™•í•˜ê²Œ í‘œì‹œ
                    formatted_code = f"\n```\n{code_text.strip()}\n```\n"
                    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ êµì²´
                    if code_block.text in final_page_text:
                        final_page_text = final_page_text.replace(code_block.text, formatted_code, 1)
        except Exception as e:
            print(f"  âš ï¸ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # íƒ­ ì½˜í…ì¸  ì²˜ë¦¬ (Google Cloud ë¬¸ì„œì˜ íƒ­ êµ¬ì¡°)
        try:
            tab_groups = content_element.find_elements(By.CSS_SELECTOR, ".devsite-tabs, [role='tablist']")

            for tab_group in tab_groups:
                tab_contents = []

                # íƒ­ ë²„íŠ¼ ì°¾ê¸°
                tab_buttons = tab_group.find_elements(By.CSS_SELECTOR, "[role='tab'], .tab-button, button[data-tab]")

                for btn in tab_buttons:
                    try:
                        tab_name = btn.text.strip() or btn.get_attribute("aria-label") or "íƒ­"

                        # íƒ­ í´ë¦­í•˜ì—¬ í™œì„±í™”
                        driver.execute_script("arguments[0].click();", btn)
                        time.sleep(0.3)

                        # í•´ë‹¹ íƒ­ì˜ íŒ¨ë„ ì°¾ê¸°
                        panel_id = btn.get_attribute("aria-controls") or btn.get_attribute("data-tab")
                        if panel_id:
                            panel = driver.find_element(By.ID, panel_id)
                        else:
                            # ë‹¤ìŒ í˜•ì œ ìš”ì†Œì—ì„œ íŒ¨ë„ ì°¾ê¸°
                            panel = driver.find_element(By.XPATH, "following-sibling::*[@role='tabpanel'][1]")

                        panel_text = panel.get_attribute("textContent").strip()
                        tab_contents.append(f"\n--- íƒ­: {tab_name} ---\n{panel_text}")
                    except Exception:
                        continue

                if tab_contents:
                    formatted_tabs = "\n".join(tab_contents)
                    # ê¸°ì¡´ íƒ­ ê·¸ë£¹ í…ìŠ¤íŠ¸ë¥¼ í¬ë§·ëœ ë²„ì „ìœ¼ë¡œ êµì²´
                    if tab_group.text:
                        final_page_text = final_page_text.replace(tab_group.text, formatted_tabs, 1)
        except Exception as e:
            print(f"  âš ï¸ íƒ­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        return final_page_text

    except Exception as e:
        print(f"  âŒ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def collect_sidebar_links(driver, wait):
    """ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë“  ë§í¬ ìˆ˜ì§‘"""
    links = set()

    # Google Cloud ë¬¸ì„œ ì‚¬ì´ë“œë°” ì…€ë ‰í„°ë“¤
    sidebar_selectors = [
        "devsite-book-nav",  # ê¸°ë³¸
        ".devsite-nav",
        "nav.devsite-book-nav",
        "[role='navigation']",
        ".devsite-section-nav",
        "#gc-sidebar"
    ]

    sidebar_found = False

    for selector in sidebar_selectors:
        try:
            sidebar = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            if sidebar:
                print(f"âœ“ ì‚¬ì´ë“œë°” ë°œê²¬: {selector}")
                sidebar_found = True

                # ì‚¬ì´ë“œë°” ë‚´ì˜ ëª¨ë“  ë§í¬ ìˆ˜ì§‘
                link_elements = sidebar.find_elements(By.TAG_NAME, "a")

                for elem in link_elements:
                    href = elem.get_attribute("href")
                    if href:
                        # BigQuery REST API ê´€ë ¨ ë§í¬ë§Œ í•„í„°ë§
                        if "/bigquery/docs/reference/rest" in href:
                            full_url = urljoin(BASE_URL, href)
                            # URL íŒŒë¼ë¯¸í„° ì œê±° (ì¤‘ë³µ ë°©ì§€)
                            clean_url = full_url.split("?")[0].split("#")[0]
                            links.add(clean_url)

                if links:
                    break

        except TimeoutException:
            continue

    if not sidebar_found:
        print("âš ï¸ ì‚¬ì´ë“œë°”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ í˜ì´ì§€ì˜ ë§í¬ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

        # í˜ì´ì§€ ì „ì²´ì—ì„œ BigQuery REST API ë§í¬ ì°¾ê¸°
        all_links = driver.find_elements(By.TAG_NAME, "a")
        for elem in all_links:
            href = elem.get_attribute("href")
            if href and "/bigquery/docs/reference/rest" in href:
                full_url = urljoin(BASE_URL, href)
                clean_url = full_url.split("?")[0].split("#")[0]
                links.add(clean_url)

    return list(links)


# ë©”ì¸ í¬ë¡¤ë§ ë¡œì§
try:
    # ì‹œì‘ í˜ì´ì§€ë¡œ ì´ë™
    full_start_url = urljoin(BASE_URL, START_URL)
    print(f"\nğŸ“ ì‹œì‘ URL: {full_start_url}")
    driver.get(full_start_url)

    # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
    time.sleep(3)

    # ì¿ í‚¤ ë™ì˜ íŒì—… ì²˜ë¦¬ (ìˆì„ ê²½ìš°)
    try:
        cookie_button = driver.find_element(By.XPATH, "//button[contains(text(), 'Accept') or contains(text(), 'ë™ì˜')]")
        cookie_button.click()
        print("ì¿ í‚¤ ë™ì˜ íŒì—…ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        time.sleep(1)
    except:
        pass

    # ì‚¬ì´ë“œë°” ë§í¬ ìˆ˜ì§‘
    print("\nğŸ” ì‚¬ì´ë“œë°”ì—ì„œ ë§í¬ë¥¼ ìˆ˜ì§‘ ì¤‘...")
    wait = WebDriverWait(driver, 15)

    urls_to_crawl = collect_sidebar_links(driver, wait)

    # ì‹œì‘ URLë„ í¬í•¨
    if full_start_url not in urls_to_crawl:
        urls_to_crawl.insert(0, full_start_url)

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    urls_to_crawl = sorted(list(set(urls_to_crawl)))

    print(f"\nâœ… ì´ {len(urls_to_crawl)}ê°œì˜ í˜ì´ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

    # í¬ë¡¤ë§í•  URL ëª©ë¡ ì¶œë ¥
    print("\nğŸ“‹ í¬ë¡¤ë§í•  í˜ì´ì§€ ëª©ë¡:")
    for i, url in enumerate(urls_to_crawl[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        print(f"  {i}. {url}")
    if len(urls_to_crawl) > 10:
        print(f"  ... ì™¸ {len(urls_to_crawl) - 10}ê°œ")

    # ê° í˜ì´ì§€ í¬ë¡¤ë§
    successful_count = 0
    failed_urls = []

    for i, url in enumerate(urls_to_crawl, 1):
        try:
            print(f"\nğŸ“„ ({i}/{len(urls_to_crawl)}) í¬ë¡¤ë§ ì¤‘: {url}")
            driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°

            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = extract_page_content(driver, url)

            if content and len(content.strip()) > 100:  # ìµœì†Œ 100ì ì´ìƒì˜ ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì €ì¥
                # íŒŒì¼ëª… ìƒì„±
                filename = clean_filename(url)
                filepath = os.path.join(OUTPUT_DIR, filename)

                # ì €ì¥í•  ë‚´ìš© êµ¬ì„±
                content_to_save = f"Source URL: {url}\n" + "=" * 80 + f"\n\n{content}"

                # íŒŒì¼ë¡œ ì €ì¥
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(content_to_save)

                print(f"  âœ… ì €ì¥ ì™„ë£Œ: {filename} ({len(content)} ë¬¸ì)")
                successful_count += 1
            else:
                print(f"  âš ï¸ ì½˜í…ì¸ ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ")
                failed_urls.append(url)

        except Exception as e:
            print(f"  âŒ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            failed_urls.append(url)

        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(1)

    # í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
    print("=" * 60)
    print(f"âœ… ì„±ê³µ: {successful_count}ê°œ í˜ì´ì§€")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ í˜ì´ì§€")

    if failed_urls:
        print("\nì‹¤íŒ¨í•œ URL ëª©ë¡:")
        for url in failed_urls[:5]:
            print(f"  - {url}")
        if len(failed_urls) > 5:
            print(f"  ... ì™¸ {len(failed_urls) - 5}ê°œ")

    print(f"\nğŸ“ ëª¨ë“  íŒŒì¼ì´ '{OUTPUT_DIR}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")

finally:
    driver.quit()
    print("\nğŸ”Œ ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
    print("âœ¨ í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")